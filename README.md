LBOR: Laplaceâ€“Beltrami Operator Regularization for Robust Skeleton-based Isolated Sign Language Recognition

This repository provides the official PyTorch implementation of the FG 2026 paper:

LBOR: Laplaceâ€“Beltrami Operator Regularization for Robust Skeleton-based Isolated Sign Language Recognition
FG 2026 â€“ IEEE International Conference on Automatic Face and Gesture Recognition

LBOR is a plug-and-play training objective for skeleton-based isolated sign language recognition (ISLR).
It directly regularizes within-class feature geometry by building class-specific subgraphs in the embedding space and minimizing a Laplacian energy, while a lightweight center-level margin preserves between-class separation.
The design is model-agnostic and can be attached to existing ISLR backbones without architectural changes.

ğŸ” Key Features

Laplaceâ€“Beltrami Operator Regularization (LBOR)

Constructs within-class kNN graphs in the feature space for each mini-batch.

Minimizes a Laplacian (Dirichlet) energy to enforce intra-class connectivity and smoothness.

Mitigates signer-driven multi-centroid fragmentation in ISLR.

Center-Level Margin Term

Encourages a margin between class centers in the embedding space.

Preserves inter-class discriminability while LBOR regularizes each class manifold.

Model-Agnostic & Lightweight

No change to the backbone architectures (HMA, SignBERT-style models, SKIM, etc.).

Implemented purely as an additional loss on top of standard classification losses.

Compatible with any skeleton-based encoder that outputs per-instance embeddings.

Reproducible Evaluation on Public ISLR Benchmarks

Word-level American Sign Language (WLASL).

NMFs-CSL (Chinese Sign Language) focusing on non-manual features.

Unified training/evaluation pipeline and configuration files for all reported experiments.

ğŸ“ Repository Structure
LBOR-FG2026/
â”œâ”€â”€ README.md
â”œâ”€â”€ LICENSE
â”œâ”€â”€ CITATION.cff
â”œâ”€â”€ FG2026_LBOR.pdf             # (Optional) Paper or preprint
â”œâ”€â”€ requirements.txt            # or environment.yml
â”œâ”€â”€ setup.py / pyproject.toml   # (Optional) install as a package
â”œâ”€â”€ .gitignore

â”œâ”€â”€ configs/                    # Experiment configurations
â”‚   â”œâ”€â”€ wlasl/
â”‚   â”‚   â”œâ”€â”€ hma_lbor.yaml
â”‚   â”‚   â”œâ”€â”€ signbert_lbor.yaml
â”‚   â”‚   â””â”€â”€ skim_lbor.yaml
â”‚   â”œâ”€â”€ nmfscsl/
â”‚   â”‚   â”œâ”€â”€ hma_lbor.yaml
â”‚   â”‚   â””â”€â”€ skim_lbor.yaml
â”‚   â””â”€â”€ default.yaml

â”œâ”€â”€ src/
â”‚   â””â”€â”€ lbor_islr/
â”‚       â”œâ”€â”€ __init__.py
â”‚       â”œâ”€â”€ models/
â”‚       â”‚   â”œâ”€â”€ hma.py
â”‚       â”‚   â”œâ”€â”€ signbert.py
â”‚       â”‚   â”œâ”€â”€ skim.py
â”‚       â”‚   â””â”€â”€ builder.py
â”‚       â”œâ”€â”€ losses/
â”‚       â”‚   â”œâ”€â”€ lbor_loss.py    # LBOR implementation (Laplacian + center margin)
â”‚       â”‚   â””â”€â”€ ce_variants.py
â”‚       â”œâ”€â”€ datasets/
â”‚       â”‚   â”œâ”€â”€ wlasl.py
â”‚       â”‚   â”œâ”€â”€ nmfscsl.py
â”‚       â”‚   â”œâ”€â”€ transforms.py
â”‚       â”‚   â””â”€â”€ utils.py
â”‚       â”œâ”€â”€ engine/
â”‚       â”‚   â”œâ”€â”€ trainer.py
â”‚       â”‚   â”œâ”€â”€ evaluator.py
â”‚       â”‚   â””â”€â”€ scheduler.py
â”‚       â”œâ”€â”€ utils/
â”‚       â”‚   â”œâ”€â”€ logger.py
â”‚       â”‚   â”œâ”€â”€ distributed.py
â”‚       â”‚   â”œâ”€â”€ seed.py
â”‚       â”‚   â””â”€â”€ misc.py
â”‚       â”œâ”€â”€ train.py            # Training entry point
â”‚       â””â”€â”€ test.py             # Evaluation entry point

â”œâ”€â”€ scripts/
â”‚   â”œâ”€â”€ train_wlasl_hma_lbor.sh
â”‚   â”œâ”€â”€ train_wlasl_signbert_lbor.sh
â”‚   â”œâ”€â”€ train_wlasl_skim_lbor.sh
â”‚   â”œâ”€â”€ train_nmfscsl_hma_lbor.sh
â”‚   â””â”€â”€ eval_wlasl_hma_lbor.sh

â”œâ”€â”€ tools/
â”‚   â”œâ”€â”€ prepare_wlasl_skeleton.py
â”‚   â”œâ”€â”€ prepare_nmfscsl_skeleton.py
â”‚   â””â”€â”€ visualize_skeleton.py

â”œâ”€â”€ docs/
â”‚   â”œâ”€â”€ INSTALL.md
â”‚   â”œâ”€â”€ DATASETS.md
â”‚   â”œâ”€â”€ EXPERIMENTS.md
â”‚   â”œâ”€â”€ METHODS.md
â”‚   â””â”€â”€ FAQ.md

â”œâ”€â”€ checkpoints/
â”‚   â””â”€â”€ README.md               # Links to pretrained weights (not stored in git)

â””â”€â”€ figures/
    â”œâ”€â”€ method_overview.png
    â”œâ”€â”€ laplacian_graph.png
    â””â”€â”€ center_margin.png


You do not need to strictly follow this layout, but a similar separation between configs, core code, scripts, tools, docs, checkpoints, and figures is recommended for clarity and reproducibility.

ğŸ“¦ Installation

We recommend using a Conda environment:

# 1. Create environment
conda create -n lbor_islr python=3.10 -y
conda activate lbor_islr

# 2. Clone this repository
git clone <repository-url>
cd LBOR-FG2026

# 3. Install dependencies
pip install -r requirements.txt

# (Optional) Install as a package
pip install -e .


The implementation has been tested with:

Python â‰¥ 3.9

PyTorch â‰¥ 1.12 (with a matching CUDA toolkit)

torchvision, PyYAML, NumPy, SciPy, tqdm, and other standard libraries

More detailed notes on environment setup and compatible versions are provided in docs/INSTALL.md.

ğŸ“š Datasets
WLASL

We evaluate LBOR on WLASL (Word-Level American Sign Language) using 2D skeleton sequences extracted from RGB videos.

Download WLASL and follow the official instructions:

WLASL website: https://dxli94.github.io/WLASL/

Extract 2D body and hand keypoints using a pose estimator such as MMPose:

# Example (pseudo-code):
python tools/extract_wlasl_poses_with_mmpose.py \
    --wlasl-root /path/to/WLASL \
    --out-root data/wlasl/poses


Organize data as:

data/
â””â”€â”€ wlasl/
    â”œâ”€â”€ poses/
    â”‚   â”œâ”€â”€ video_000001.npy
    â”‚   â”œâ”€â”€ video_000002.npy
    â”‚   â””â”€â”€ ...
    â”œâ”€â”€ wlasl_train_list.txt
    â”œâ”€â”€ wlasl_val_list.txt
    â””â”€â”€ wlasl_test_list.txt


Optionally, run the preparation script to convert raw pose files into the exact format used by the datasets module:

python tools/prepare_wlasl_skeleton.py \
    --raw-root data/wlasl/poses \
    --out-root data/wlasl

NMFs-CSL

NMFs-CSL is a Chinese Sign Language dataset designed to emphasize non-manual features (facial expressions, mouth shapes, etc.).
We follow the official split and use pre-extracted skeletal sequences when available.

If the dataset is not directly accessible, please refer to docs/DATASETS.md for:

The expected directory structure and file naming convention.

How to adapt your own CSL data into the same skeleton format.

Note: Some corpora mentioned in the paper (e.g., SLR500, MS-ASL) are not used in our experiments due to limited or unstable public availability, which prevents fair and fully reproducible comparison.

âš™ï¸ Configuration

All experiment settings are specified via YAML configuration files under configs/.

A typical configuration (e.g., configs/wlasl/hma_lbor.yaml) contains:

Dataset

Dataset name, root directory, split files.

Number of classes, number of joints, number of frames (we resample each clip to a fixed number of frames).

Model

Backbone type: hma, signbert, skim.

Embedding dimension (feature dimension).

Dropout rate, layer counts, and other architecture hyperparameters.

Loss (LBOR)

lambda_lap: weight of the within-class Laplacian term.

mu_margin: weight of the center-level margin term.

margin_M: desired squared Euclidean margin between class centers.

tau: temperature in the Gaussian kernel for graph edge weights.

use_knn: whether to sparsify edges using within-class kNN.

knn_k: number of neighbors when use_knn is enabled.

Training

Number of epochs, batch size, optimizer type (e.g., AdamW), base learning rate, weight decay.

Learning rate scheduler (e.g., cosine decay) and warmup epochs.

Augmentation

Temporal resampling strategy, random cropping, random flipping/scaling.

Skeleton normalization (translation, scaling) and joint selection.

Misc

Random seed, number of dataloader workers.

Output directory for logs and checkpoints.

Checkpoint saving frequency.

You can edit these YAML files directly or override specific options from the command line.

ğŸš€ Training

After installing dependencies and preparing datasets, training LBOR on WLASL with a chosen backbone can be done using the provided scripts:

# Example: HMA backbone + LBOR on WLASL
bash scripts/train_wlasl_hma_lbor.sh

# Example: SignBERT-style backbone + LBOR on WLASL
bash scripts/train_wlasl_signbert_lbor.sh

# Example: SKIM backbone + LBOR on WLASL
bash scripts/train_wlasl_skim_lbor.sh

# Example: HMA backbone + LBOR on NMFs-CSL
bash scripts/train_nmfscsl_hma_lbor.sh


Each script internally calls the training entry point:

CUDA_VISIBLE_DEVICES=0,1 python -m lbor_islr.train \
    --config configs/wlasl/hma_lbor.yaml


Main command-line arguments (see src/lbor_islr/train.py):

--config: path to the YAML config file.

--resume: path to a checkpoint to resume training (optional).

--seed: optional override of the random seed.

--output-dir: optional override of the output directory.

Training logs (including classification loss, Laplacian loss, margin loss, Top-1/Top-5 accuracy) will be stored under the configured output directory.

âœ… Evaluation

To evaluate a trained model on the validation or test set:

# Example: evaluation of HMA + LBOR on WLASL
bash scripts/eval_wlasl_hma_lbor.sh


or directly:

CUDA_VISIBLE_DEVICES=0 python -m lbor_islr.test \
    --config configs/wlasl/hma_lbor.yaml \
    --checkpoint checkpoints/wlasl/hma_lbor_best.pth


The evaluation script reports:

Top-1 and Top-5 accuracy.

Optionally, mean class accuracy and per-class metrics.

Optionally, confusion matrices and t-SNE visualizations of the learned embeddings (config-dependent).

